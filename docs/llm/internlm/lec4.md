---
counter: true
comment: true
---

# XTuner å¤§æ¨¡å‹å•å¡ä½æˆæœ¬å¾®è°ƒå®æˆ˜

!!! abstract
    å­¦ä¹ è‡ª [XTuner å¤§æ¨¡å‹å•å¡ä½æˆæœ¬å¾®è°ƒå®æˆ˜](https://www.bilibili.com/video/BV1yK4y1B75J)


## Finetune

> LLM çš„ä¸‹æ¸¸åº”ç”¨ä¸­ï¼Œ**å¢é‡é¢„è®­ç»ƒ**å’Œ**æŒ‡ä»¤è·Ÿéš**æ˜¯ç»å¸¸ä¼šç”¨åˆ°ä¸¤ç§çš„å¾®è°ƒæ¨¡å¼

### å¢é‡é¢„è®­ç»ƒå¾®è°ƒ

- ä½¿ç”¨åœºæ™¯ï¼šè®©åŸºåº§æ¨¡å‹å­¦ä¹ åˆ°ä¸€äº›æ–°çŸ¥è¯†ï¼Œå¦‚æŸä¸ªå‚ç±»é¢†åŸŸçš„å¸¸è¯†
- è®­ç»ƒæ•°æ®ï¼šæ–‡ç« ã€ä¹¦ç±ã€ä»£ç ç­‰

- ä¸ºäº†è®© LLM çŸ¥é“ä»€ä¹ˆæ—¶å€™å¼€å§‹ä¸€æ®µè¯ï¼Œä»€ä¹ˆæ—¶å€™ç»“æŸä¸€æ®µè¯ï¼Œå®é™…è®­ç»ƒæ—¶éœ€è¦å¯¹æ•°æ®æ·»åŠ èµ·å§‹ç¬¦ï¼ˆBOSï¼‰å’Œç»“æŸç¬¦ï¼ˆEOSï¼‰ï¼Œå¤§å¤šæ•°çš„æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨ `<s>` ä½œä¸ºèµ·å§‹ç¬¦ï¼Œ`</s>` ä½œä¸ºç»“æŸç¬¦


### æŒ‡ä»¤è·Ÿéšå¾®è°ƒ

- ä½¿ç”¨åœºæ™¯ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯æ¨¡ç‰ˆï¼Œæ ¹æ®äººç±»æŒ‡ä»¤è¿›è¡Œå¯¹è¯
- è®­ç»ƒæ•°æ®ï¼šé«˜è´¨é‡çš„å¯¹è¯ï¼Œé—®ç­”æ•°æ®

!!! example ""
    åœ¨å®é™…å¯¹è¯ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸‰ç§è§’è‰²

    - `System` ç»™å®šä¸€äº›ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚â€œä½ æ˜¯ä¸€ä¸ªå®‰å…¨çš„ AI åŠ©æ‰‹â€
    - `User` å®é™…ç”¨æˆ·ï¼Œä¼šæå‡ºä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ â€œä¸–ç•Œç¬¬ä¸€é«˜å³°æ˜¯â€
    - `Assistant` æ ¹æ® User çš„è¾“å…¥ï¼Œç»“åˆ System çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåšå‡ºå›ç­”ï¼Œæ¯”å¦‚ â€œç ç©†æœ—ç›å³°â€

    **åœ¨ä½¿ç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œé€šå¸¸æ˜¯ä¸ä¼šæ„ŸçŸ¥åˆ°è¿™ä¸‰ç§è§’è‰²**

- å¯¹è¯æ¨¡ç‰ˆ

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/ft-1-chat-template.jpg"></center>

- ä¸åŒäºå¢é‡é¢„è®­ç»ƒå¾®è°ƒï¼Œæ•°æ®ä¸­ä¼šæœ‰ Input å’Œ Output å¸Œæœ›æ¨¡å‹å­¦ä¼šçš„æ˜¯ç­”æ¡ˆï¼ˆOutputï¼‰ï¼Œè€Œä¸æ˜¯é—®é¢˜ï¼ˆInputï¼‰ï¼Œè®­ç»ƒæ—¶åªä¼šå¯¹ç­”æ¡ˆï¼ˆOutputï¼‰éƒ¨åˆ†è®¡ç®— Loss
- è®­ç»ƒæ—¶ï¼Œä¼šå’Œæ¨ç†æ—¶ä¿æŒä¸€è‡´ï¼Œå¯¹æ•°æ®æ·»åŠ ç›¸å¯¹åº”çš„å¯¹è¯æ¨¡ç‰ˆ


## LoRA & QLoRA

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/LoRA-QLoRA.jpg"></center>

## Xtuner

- **å‚»ç“œåŒ–**ï¼šä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯
- **è½»é‡çº§**ï¼šå¯¹äº 7B å‚æ•°é‡çš„ LLMï¼Œå¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB

### å¿«é€Ÿä¸Šæ‰‹

#### ä½¿ç”¨æµç¨‹

1. å®‰è£…

```bash
pip install xtuner
```

2. æŒ‘é€‰é…ç½®æ¨¡ç‰ˆ

```bash
xtuner list-cfg -p internlm_20b
```

3. ä¸€é”®è®­ç»ƒ

```bash
xtuner train internlm_20b_qlora_oasst1_512_e3
```

| Config   | Description                           |
| -------- | ------------------------------------- |
| æ¨¡å‹å   | internlm_20b (æ—  chat ä»£è¡¨æ˜¯åŸºåº§æ¨¡å‹) |
| ä½¿ç”¨ç®—æ³• | qlora                                 |
| æ•°æ®é›†   | oasst1                                |
| æ•°æ®é•¿åº¦ | 512                                   |
| Epoch    | e3, epoch 3                           |

#### è‡ªå®šä¹‰è®­ç»ƒ

1. æ‹·è´é…ç½®æ¨¡ç‰ˆ
2. ä¿®æ”¹é…ç½®æ¨¡ç‰ˆ
3. å¯åŠ¨è®­ç»ƒ

| å¸¸ç”¨è¶…å‚            | Description                                            |
| ------------------- | ------------------------------------------------------ |
| data_path           | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                          |
| max_length          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                      |
| pack_to_max_length  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡     |
| accumulative_counts | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°               |
| evaluation_inputs   | è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šæ ¹æ®ç»™å®šçš„é—®é¢˜è¿›è¡Œæ¨ç†ï¼Œä¾¿äºè§‚æµ‹è®­ç»ƒçŠ¶æ€ |
| evaluation_freq     | Evaluation çš„æµ‹è¯„é—´éš” iter æ•°                          |

#### å¯¹è¯

ä¸ºäº†ä¾¿äºå¼€å‘è€…æŸ¥çœ‹è®­ç»ƒæ•ˆæœï¼ŒXtuner æä¾›äº†ä¸€é”®å¯¹è¯å€Ÿå£

1. Float16 æ¨¡å‹å¯¹è¯

```bash
xtuner chat internlm/internlm-chat-20b
```

2. 4bits æ¨¡å‹å¯¹è¯

```bash
xtuner chat internlm/internlm-chat-20b --bits 4
```

3. åŠ è½½ Adapter æ¨¡å‹å¯¹è¯

```bash
xtuner chat internlm/internlm-chat-20b --adapter $ADAPTER_DIR
```

åŒæ—¶ï¼ŒXtuner è¿˜æ”¯æŒå·¥å…·ç±»æ¨¡å‹çš„å¯¹è¯

### XTuner æ•°æ®å¼•æ“

#### æ•°æ®å¤„ç†æµç¨‹

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/xunter-pipe-1.jpg"></center>

#### å¤šæ•°æ®æ ·æœ¬æ‹¼æ¥ï¼ˆPack Datasetï¼‰

å¢å¼ºå¹¶è¡Œæ€§ï¼Œå……åˆ†åˆ©ç”¨ GPU èµ„æºï¼

## 8 GB æ˜¾å¡ç©è½¬ LLM

### XTuner åŠ é€Ÿæ–¹å¼

- `Flash Attention`ï¼š`Flash Attention` å°† `Attention` è®¡ç®—å¹¶è¡ŒåŒ–ï¼Œé¿å…äº†è®¡ç®—è¿‡ç¨‹ä¸­ `Attention Score NxN` çš„æ˜¾å­˜å ç”¨ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„ N éƒ½æ¯”è¾ƒå¤§ï¼‰

- `DeepSpeed ZeRO`
    - `ZeRO` ä¼˜åŒ–ï¼Œé€šè¿‡å°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ‡ç‰‡ä¿å­˜ï¼Œèƒ½å¤Ÿåœ¨å¤š GPU è®­ç»ƒæ—¶æ˜¾è‘—èŠ‚çœæ˜¾å­˜
    - é™¤äº†å°†è®­ç»ƒä¸­é—´çŠ¶æ€åˆ‡ç‰‡å¤–ï¼Œ`DeepSpeed` è®­ç»ƒæ—¶ä½¿ç”¨ FP16 çš„æƒé‡ï¼Œç›¸è¾ƒäº `Pytorch` çš„ `AMP` è®­ç»ƒï¼Œåœ¨å• GPU ä¸Šä¹Ÿèƒ½å¤§å¹…èŠ‚çœæ˜¾å­˜

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/fa-dz.jpg"></center>

`DeepSpeed` ä¸ `Flash Attention` è™½ç„¶èƒ½å¤Ÿå¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ï¼Œä½†ä½¿ç”¨é—¨æ§›ç›¸å¯¹è¾ƒé«˜ï¼Œéœ€è¦å¤æ‚çš„é…ç½®ï¼Œç”šè‡³ä¿®æ”¹ä»£ç ï¼Œä¸ºäº†è®©å¼€å‘è€…ä¸“æ³¨äºæ•°æ®ï¼ŒXTuner ä¼šè‡ªåŠ¨ `dispatch Flash Attention`ï¼Œå¹¶ä¸€é”®å¯åŠ¨ `DeepSpeed ZeRO`

```bash
xtuner train internlm_20b_qlora_oasst1_512_e3 \
    --deepspeed deepspeed_zero3
```

## å®æˆ˜è®­ç»ƒ

### ç¯å¢ƒ & å®‰è£…

- ç¯å¢ƒï¼šUbuntu + Anaconda + CUDA/CUDNN + 8GB Nvidia GPU
- å®‰è£…ï¼š

```bash
conda create --name xtuner0.1.9 python=3.10 -y
conda activate xtuner0.1.9
cd ~ && mkdir xtuner019 && cd xtuner019
git clone -b v0.1.9  https://github.com/InternLM/xtuner
cd xtuner
pip install -e '.[all]'
```

- åˆ›å»ºæ•°æ®é›†å·¥ä½œè·¯å¾„ï¼š

```bash
mkdir ~/ft-oasst1 && cd ~/ft-oasst1
```

### å¾®è°ƒ

#### å‡†å¤‡é…ç½®æ–‡ä»¶

```bash
cd ~/ft-oasst1
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

#### æ¨¡å‹ä¸‹è½½

- InternStudio å¹³å°ä¸Š

```bash
cp -r /root/share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/
```
- ä¸ç”¨ xtuner é»˜è®¤çš„ä» huggingface æ‹‰å–æ¨¡å‹ï¼Œè€Œæ˜¯æå‰ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°

```bash
# åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œæ”¾æ¨¡å‹æ–‡ä»¶ï¼Œé˜²æ­¢æ•£è½ä¸€åœ°
mkdir ~/ft-oasst1/internlm-chat-7b

# è£…ä¸€ä¸‹æ‹‰å–æ¨¡å‹æ–‡ä»¶è¦ç”¨çš„åº“
pip install modelscope

# ä» modelscope ä¸‹è½½ä¸‹è½½æ¨¡å‹æ–‡ä»¶
cd ~/ft-oasst1
apt install git git-lfs -y
git lfs install
git lfs clone https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3
```
#### æ•°æ®é›†ä¸‹è½½

> https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main


#### ä¿®æ”¹é…ç½®æ–‡ä»¶

ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ä¸º æœ¬åœ°è·¯å¾„

```bash
cd ~/ft-oasst1
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```

```diff
# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®é›†ä¸ºæœ¬åœ°è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = './openassistant-guanaco'
```

#### å¼€å§‹å¾®è°ƒ

```bash
# å•å¡
## ç”¨åˆšæ‰æ”¹å¥½çš„configæ–‡ä»¶è®­ç»ƒ
xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# å¤šå¡
NPROC_PER_NODE=${GPU_NUM} xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# è‹¥è¦å¼€å¯ deepspeed åŠ é€Ÿï¼Œå¢åŠ  --deepspeed deepspeed_zero2 å³å¯
```

> å¾®è°ƒå¾—åˆ°çš„ PTH æ¨¡å‹æ–‡ä»¶å’Œå…¶ä»–æ‚ä¸ƒæ‚å…«çš„æ–‡ä»¶éƒ½é»˜è®¤åœ¨å½“å‰çš„ `./work_dirs` ä¸­ã€‚

#### ç”Ÿæˆ Adapter æ–‡ä»¶

> å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œå³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹
> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

```bash
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1

xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf
```


### éƒ¨ç½²ä¸æµ‹è¯•

#### å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹

```bash
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

####  ä¸åˆå¹¶åçš„æ¨¡å‹å¯¹è¯

```bash
# åŠ è½½ Adapter æ¨¡å‹å¯¹è¯ï¼ˆFloat 16ï¼‰
xtuner chat ./merged --prompt-template internlm_chat

# 4 bit é‡åŒ–åŠ è½½
# xtuner chat ./merged --bits 4 --prompt-template internlm_chat
```


| å¾®è°ƒå‰                                                                                 | å¾®è°ƒå                                                                                |
| -------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| <center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/ft-before.png"></center> | <center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/ft-after.png"></center> |

## è‡ªå®šä¹‰å¾®è°ƒ

> ä»¥ **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†**ä¸ºä¾‹

### æ¦‚è¿°

#### **åœºæ™¯éœ€æ±‚**

   åŸºäº InternLM-chat-7B æ¨¡å‹ï¼Œç”¨ MedQA æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶å¾€`åŒ»å­¦é—®ç­”`é¢†åŸŸå¯¹é½ã€‚

#### **çœŸå®æ•°æ®é¢„è§ˆ**

| é—®é¢˜                                                       | ç­”æ¡ˆ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What are ketorolac eye drops?ï¼ˆä»€ä¹ˆæ˜¯é…®å’¯é…¸æ»´çœ¼æ¶²ï¼Ÿï¼‰      | Ophthalmic   ketorolac is used to treat itchy eyes caused by allergies. It also is used to   treat swelling and redness (inflammation) that can occur after cataract   surgery. Ketorolac is in a class of medications called nonsteroidal   anti-inflammatory drugs (NSAIDs). It works by stopping the release of   substances that cause allergy symptoms and inflammation.                                                                                                                                                                                                                                                                                                                                                           |
| What medicines raise blood sugar? ï¼ˆä»€ä¹ˆè¯ç‰©ä¼šå‡é«˜è¡€ç³–ï¼Ÿï¼‰ | Some   medicines for conditions other than diabetes can raise your blood sugar   level. This is a concern when you have diabetes. Make sure every doctor you   see knows about all of the medicines, vitamins, or herbal supplements you   take. This means anything you take with or without a prescription. Examples include:     Barbiturates.     Thiazide diuretics.     Corticosteroids.     Birth control pills (oral contraceptives) and progesterone.     Catecholamines.     Decongestants that contain beta-adrenergic agents, such as pseudoephedrine.     The B vitamin niacin. The risk of high blood sugar from niacin lowers after you have taken it for a few months. The antipsychotic medicine olanzapine (Zyprexa). |

### æ•°æ®å‡†å¤‡ 

> **ä»¥** **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†ä¸ºä¾‹**

**åŸæ ¼å¼ï¼š(.xlsx)**

| **é—®é¢˜** | è¯ç‰©ç±»å‹ | é—®é¢˜ç±»å‹ | **å›ç­”** | ä¸»é¢˜ | URL |
| -------- | -------- | -------- | -------- | ---- | --- |
| aaa      | bbb      | ccc      | ddd      | eee  | fff |

#### å°†æ•°æ®è½¬ä¸º XTuner çš„æ•°æ®æ ¼å¼

**ç›®æ ‡æ ¼å¼ï¼š(.jsonL)**

```JSON
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
```

ğŸ§ é€šè¿‡ pythonè„šæœ¬ï¼šå°† `.xlsx` ä¸­çš„ é—®é¢˜ å’Œ å›ç­” ä¸¤åˆ— æå–å‡ºæ¥ï¼Œå†æ”¾å…¥ `.jsonL` æ–‡ä»¶çš„æ¯ä¸ª conversation çš„ input å’Œ output ä¸­ã€‚

> è¿™ä¸€æ­¥çš„ python è„šæœ¬å¯ä»¥è¯· ChatGPT æ¥å®Œæˆã€‚

```text
Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx
Step1: The input file is .xlsx. Exact the column A and column D in the sheet named "DrugQA" .
Step2: Put each value in column A into each "input" of each "conversation". Put each value in column D into each "output" of each "conversation".
Step3: The output file is .jsonL. It looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step4: All "system" value changes to "You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients' questions."
```

> ChatGPT ç”Ÿæˆçš„ python ä»£ç å¦‚ä¸‹ï¼š

??? note "xlsx2jsonl.py"
    ```python
    import openpyxl
    import json

    def process_excel_to_json(input_file, output_file):
        # Load the workbook
        wb = openpyxl.load_workbook(input_file)

        # Select the "DrugQA" sheet
        sheet = wb["DrugQA"]

        # Initialize the output data structure
        output_data = []

        # Iterate through each row in column A and D
        for row in sheet.iter_rows(min_row=2, max_col=4, values_only=True):
            system_value = "You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients' questions."

            # Create the conversation dictionary
            conversation = {
                "system": system_value,
                "input": row[0],
                "output": row[3]
            }

            # Append the conversation to the output data
            output_data.append({"conversation": [conversation]})

        # Write the output data to a JSON file
        with open(output_file, 'w', encoding='utf-8') as json_file:
            json.dump(output_data, json_file, indent=4)

        print(f"Conversion complete. Output written to {output_file}")

    # Replace 'MedQA2019.xlsx' and 'output.jsonl' with your actual input and output file names
    process_excel_to_json('MedQA2019.xlsx', 'output.jsonl')
    ```

æ‰§è¡Œ python è„šæœ¬ï¼Œè·å¾—æ ¼å¼åŒ–åçš„æ•°æ®é›†ï¼š
```bash
python xlsx2jsonl.py
```

**æ ¼å¼åŒ–åçš„æ•°æ®é›†é•¿è¿™æ ·ï¼š**

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/data-processed.png"></center>

æ­¤æ—¶ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å‰²ï¼ŒåŒæ ·å¯ä»¥è®© ChatGPT å†™ python ä»£ç ã€‚å½“ç„¶å¦‚æœä½ æ²¡æœ‰ä¸¥æ ¼çš„ç§‘ç ”éœ€æ±‚ã€ä¸åœ¨ä¹â€œè®­ç»ƒé›†æ³„éœ²â€çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä¸åšè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„åˆ†å‰²ã€‚

#### åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

```text
my .jsonL file looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step1, read the .jsonL file.
Step2, count the amount of the "conversation" elements.
Step3, randomly split all "conversation" elements by 7:3. Targeted structure is same as the input.
Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl
```

> ç”Ÿæˆçš„ python ä»£ç å¦‚ä¸‹ï¼š

??? note "split2train_and_test.py"
    ```python
    import json
    import random

    def split_conversations(input_file, train_output_file, test_output_file):
        # Read the input JSONL file
        with open(input_file, 'r', encoding='utf-8') as jsonl_file:
            data = json.load(jsonl_file)

        # Count the number of conversation elements
        num_conversations = len(data)
        
        # Shuffle the data randomly
        random.shuffle(data)
        random.shuffle(data)
        random.shuffle(data)

        # Calculate the split points for train and test
        split_point = int(num_conversations * 0.7)

        # Split the data into train and test
        train_data = data[:split_point]
        test_data = data[split_point:]

        # Write the train data to a new JSONL file
        with open(train_output_file, 'w', encoding='utf-8') as train_jsonl_file:
            json.dump(train_data, train_jsonl_file, indent=4)

        # Write the test data to a new JSONL file
        with open(test_output_file, 'w', encoding='utf-8') as test_jsonl_file:
            json.dump(test_data, test_jsonl_file, indent=4)

        print(f"Split complete. Train data written to {train_output_file}, Test data written to {test_output_file}")

    # Replace 'input.jsonl', 'train.jsonl', and 'test.jsonl' with your actual file names
    split_conversations('MedQA2019-structured.jsonl', 'MedQA2019-structured-train.jsonl', 'MedQA2019-structured-test.jsonl')
    ```


### å¼€å§‹è‡ªå®šä¹‰å¾®è°ƒ

æ­¤æ—¶ï¼Œæˆ‘ä»¬é‡æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥ç©â€œå¾®è°ƒè‡ªå®šä¹‰æ•°æ®é›†â€
```bash
mkdir ~/ft-medqa && cd ~/ft-medqa
```

æŠŠå‰é¢ä¸‹è½½å¥½çš„internlm-chat-7bæ¨¡å‹æ–‡ä»¶å¤¹æ‹·è´è¿‡æ¥ã€‚

```bash
cp -r ~/ft-oasst1/internlm-chat-7b .
```
åˆ«å¿˜äº†æŠŠè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå³å‡ ä¸ª `.jsonL`ï¼Œä¹Ÿä¼ åˆ°æœåŠ¡å™¨ä¸Šã€‚

```bash
git clone https://github.com/InternLM/tutorial
```

```bash
cp ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .
```



#### å‡†å¤‡é…ç½®æ–‡ä»¶
```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
# æ”¹ä¸ªæ–‡ä»¶å
mv internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py

# ä¿®æ”¹é…ç½®æ–‡ä»¶å†…å®¹
vim internlm_chat_7b_qlora_medqa2019_e3.py
```

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚
```diff
# ä¿®æ”¹importéƒ¨åˆ†
- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import template_map_fn_factory

# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸º MedQA2019-structured-train.jsonl è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'MedQA2019-structured-train.jsonl'

# ä¿®æ”¹ train_dataset å¯¹è±¡
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=alpaca_map_fn,
+   dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)
```
#### XTunerï¼å¯åŠ¨ï¼

```bash
xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2
```

#### ç”Ÿæˆ Adapter æ–‡ä»¶

#### éƒ¨ç½²ä¸æµ‹è¯•


## ç”¨ MS-Agent æ•°æ®é›†èµ‹äºˆ LLM ä»¥ Agent èƒ½åŠ›

### æ¦‚è¿°

MSAgent æ•°æ®é›†æ¯æ¡æ ·æœ¬åŒ…å«ä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼ˆconversationsï¼‰ï¼Œå…¶é‡Œé¢åŒ…å«äº† systemã€userã€assistant ä¸‰ç§å­—æ®µã€‚å…¶ä¸­ï¼š

- system: è¡¨ç¤ºç»™æ¨¡å‹å‰ç½®çš„äººè®¾è¾“å…¥ï¼Œå…¶ä¸­æœ‰å‘Šè¯‰æ¨¡å‹å¦‚ä½•è°ƒç”¨æ’ä»¶ä»¥åŠç”Ÿæˆè¯·æ±‚

- user: è¡¨ç¤ºç”¨æˆ·çš„è¾“å…¥ promptï¼Œåˆ†ä¸ºä¸¤ç§ï¼Œé€šç”¨ç”Ÿæˆçš„promptå’Œè°ƒç”¨æ’ä»¶éœ€æ±‚çš„ prompt

- assistant: ä¸ºæ¨¡å‹çš„å›å¤ã€‚å…¶ä¸­ä¼šåŒ…æ‹¬æ’ä»¶è°ƒç”¨ä»£ç å’Œæ‰§è¡Œä»£ç ï¼Œè°ƒç”¨ä»£ç æ˜¯è¦ LLM ç”Ÿæˆçš„ï¼Œè€Œæ‰§è¡Œä»£ç æ˜¯è°ƒç”¨æœåŠ¡æ¥ç”Ÿæˆç»“æœçš„

ä¸€æ¡è°ƒç”¨ç½‘é¡µæœç´¢æ’ä»¶æŸ¥è¯¢â€œä¸Šæµ·æ˜å¤©å¤©æ°”â€çš„æ•°æ®æ ·æœ¬ç¤ºä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/msagent_data.png"></center>


### å¾®è°ƒæ­¥éª¤

#### å‡†å¤‡å·¥ä½œ
> xtuner æ˜¯ä»å›½å†…çš„ ModelScope å¹³å°ä¸‹è½½ MS-Agent æ•°æ®é›†ï¼Œå› æ­¤ä¸ç”¨æå‰æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†æ–‡ä»¶ã€‚

```bash
# å‡†å¤‡å·¥ä½œ
mkdir ~/ft-msagent && cd ~/ft-msagent
cp -r ~/ft-oasst1/internlm-chat-7b .

# æŸ¥çœ‹é…ç½®æ–‡ä»¶
xtuner list-cfg | grep msagent

# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_7b_qlora_msagent_react_e3_gpu8 .

# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
vim ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py 
```

```diff
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'
```

#### å¼€å§‹å¾®è°ƒ
```Bash
xtuner train ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2
```

### ç›´æ¥ä½¿ç”¨

> ç”±äº msagent çš„è®­ç»ƒéå¸¸è´¹æ—¶ï¼Œå¤§å®¶å¦‚æœæƒ³å°½å¿«æŠŠè¿™ä¸ªæ•™ç¨‹è·Ÿå®Œï¼Œå¯ä»¥ç›´æ¥ä» modelScope æ‹‰å–å’±ä»¬å·²ç»å¾®è°ƒå¥½äº†çš„ Adapterã€‚å¦‚ä¸‹æ¼”ç¤ºã€‚

#### ä¸‹è½½ Adapter
```Bash
cd ~/ft-msagent
apt install git git-lfs
git lfs install
git lfs clone https://www.modelscope.cn/xtuner/internlm-7b-qlora-msagent-react.git
```

æœ‰äº†è¿™ä¸ªåœ¨ msagent ä¸Šè®­ç»ƒå¾—åˆ°çš„Adapterï¼Œæ¨¡å‹ç°åœ¨å·²ç»æœ‰ agent èƒ½åŠ›äº†ï¼å°±å¯ä»¥åŠ  --lagent ä»¥è°ƒç”¨æ¥è‡ª lagent çš„ä»£ç†åŠŸèƒ½äº†ï¼

#### æ·»åŠ  serper ç¯å¢ƒå˜é‡

> **å¼€å§‹ chat ä¹‹å‰ï¼Œè¿˜è¦åŠ ä¸ª serper çš„ç¯å¢ƒå˜é‡ï¼š**
> 
> å» serper.dev å…è´¹æ³¨å†Œä¸€ä¸ªè´¦å·ï¼Œç”Ÿæˆè‡ªå·±çš„ api keyã€‚è¿™ä¸ªä¸œè¥¿æ˜¯ç”¨æ¥ç»™ lagent å»è·å– google æœç´¢çš„ç»“æœçš„ã€‚ç­‰äºæ˜¯ serper.dev å¸®ä½ å»è®¿é—® googleï¼Œè€Œä¸æ˜¯ä»ä½ è‡ªå·±æœ¬åœ°å»è®¿é—® google äº†ã€‚

æ·»åŠ  serper api key åˆ°ç¯å¢ƒå˜é‡ï¼š

```bash
export SERPER_API_KEY=abcdefg
```

#### xtuner + agentï¼Œå¯åŠ¨ï¼

```bash
xtuner chat ./internlm-chat-7b --adapter internlm-7b-qlora-msagent-react --lagent --prompt-template internlm_chat
```


## ä½œä¸šï¼šXTuner InternLM-Chat ä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥å¾®è°ƒå®è·µ

### ç¯å¢ƒå‡†å¤‡

```bash
# InternStudio å¹³å°ä¸­ï¼Œä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch 2.0.1 çš„ç¯å¢ƒï¼ˆåç»­å‡åœ¨è¯¥ç¯å¢ƒæ‰§è¡Œï¼Œè‹¥ä¸ºå…¶ä»–ç¯å¢ƒå¯ä½œä¸ºå‚è€ƒï¼‰
# è¿›å…¥ç¯å¢ƒåé¦–å…ˆ bash
# è¿›å…¥ç¯å¢ƒåé¦–å…ˆ bash
# è¿›å…¥ç¯å¢ƒåé¦–å…ˆ bash
bash
conda create --name personal_assistant --clone=/root/share/conda_envs/internlm-base
# å¦‚æœåœ¨å…¶ä»–å¹³å°ï¼š
# conda create --name personal_assistant python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate personal_assistant
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
# personal_assistantç”¨äºå­˜æ”¾æœ¬æ•™ç¨‹æ‰€ä½¿ç”¨çš„ä¸œè¥¿
mkdir /root/personal_assistant && cd /root/personal_assistant
mkdir /root/personal_assistant/xtuner019 && cd /root/personal_assistant/xtuner019

# æ‹‰å– 0.1.9 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```

### æ•°æ®å‡†å¤‡

```bash
mkdir -p /root/personal_assistant/data && cd /root/personal_assistant/data
```

åœ¨ `data` ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªjsonæ–‡ä»¶ `personal_assistant.json` ä½œä¸ºæœ¬æ¬¡å¾®è°ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ã€‚

å…¶ä¸­ `conversation` è¡¨ç¤ºä¸€æ¬¡å¯¹è¯çš„å†…å®¹ï¼Œ`input` ä¸ºè¾“å…¥ï¼Œå³ç”¨æˆ·ä¼šé—®çš„é—®é¢˜ï¼Œ`output` ä¸ºè¾“å‡ºï¼Œå³æƒ³è¦æ¨¡å‹å›ç­”çš„ç­”æ¡ˆã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªpythonè„šæœ¬ï¼Œç”¨äºç”Ÿæˆæ•°æ®é›†ã€‚

??? note "generate_data.py"
    ```python
    import json

    # è¾“å…¥ä½ çš„åå­—
    name = 'ç‹™å‡»ç¾ä½'
    # é‡å¤æ¬¡æ•°
    n = 10000

    data = [
        {
            "conversation": [
                {
                    "input": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»",
                    "output": "æˆ‘æ˜¯{}çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦".format(name)
                }
            ]
        }
    ]

    for i in range(n):
        data.append(data[0])

    with open('personal_assistant.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    ```

### é…ç½®å‡†å¤‡

```bash
mkdir -p /root/personal_assistant/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/personal_assistant/model/Shanghai_AI_Laboratory
```

```bash
# åˆ›å»ºç”¨äºå­˜æ”¾é…ç½®çš„æ–‡ä»¶å¤¹configå¹¶è¿›å…¥
mkdir /root/personal_assistant/config && cd /root/personal_assistant/config
# æ‹·è´ä¸€ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```
ä¿®æ”¹æ‹·è´åçš„æ–‡ä»¶ï¼š

??? note "internlm_chat_7b_qlora_oasst1_e3_copy.py"
    ```python
    # Copyright (c) OpenMMLab. All rights reserved.
    import torch
    from bitsandbytes.optim import PagedAdamW32bit
    from datasets import load_dataset
    from mmengine.dataset import DefaultSampler
    from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                                LoggerHook, ParamSchedulerHook)
    from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR
    from peft import LoraConfig
    from transformers import (AutoModelForCausalLM, AutoTokenizer,
                            BitsAndBytesConfig)

    from xtuner.dataset import process_hf_dataset
    from xtuner.dataset.collate_fns import default_collate_fn
    from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
    from xtuner.engine import DatasetInfoHook, EvaluateChatHook
    from xtuner.model import SupervisedFinetune
    from xtuner.utils import PROMPT_TEMPLATE

    #######################################################################
    #                          PART 1  Settings                           #
    #######################################################################
    # Model
    pretrained_model_name_or_path = '/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b'

    # Data
    data_path = '/root/personal_assistant/data/personal_assistant.json'
    prompt_template = PROMPT_TEMPLATE.internlm_chat
    max_length = 512
    pack_to_max_length = True

    # Scheduler & Optimizer
    batch_size = 2  # per_device
    accumulative_counts = 16
    dataloader_num_workers = 0
    max_epochs = 3
    optim_type = PagedAdamW32bit
    lr = 2e-4
    betas = (0.9, 0.999)
    weight_decay = 0
    max_norm = 1  # grad clip

    # Evaluate the generation performance during the training
    evaluation_freq = 90
    SYSTEM = ''
    evaluation_inputs = [ 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»' ]

    #######################################################################
    #                      PART 2  Model & Tokenizer                      #
    #######################################################################
    tokenizer = dict(
        type=AutoTokenizer.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        padding_side='right')

    model = dict(
        type=SupervisedFinetune,
        llm=dict(
            type=AutoModelForCausalLM.from_pretrained,
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            quantization_config=dict(
                type=BitsAndBytesConfig,
                load_in_4bit=True,
                load_in_8bit=False,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type='nf4')),
        lora=dict(
            type=LoraConfig,
            r=64,
            lora_alpha=16,
            lora_dropout=0.1,
            bias='none',
            task_type='CAUSAL_LM'))

    #######################################################################
    #                      PART 3  Dataset & Dataloader                   #
    #######################################################################
    train_dataset = dict(
        type=process_hf_dataset,
        dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
        tokenizer=tokenizer,
        max_length=max_length,
        dataset_map_fn=None,
        template_map_fn=dict(
            type=template_map_fn_factory, template=prompt_template),
        remove_unused_columns=True,
        shuffle_before_pack=True,
        pack_to_max_length=pack_to_max_length)

    train_dataloader = dict(
        batch_size=batch_size,
        num_workers=dataloader_num_workers,
        dataset=train_dataset,
        sampler=dict(type=DefaultSampler, shuffle=True),
        collate_fn=dict(type=default_collate_fn))

    #######################################################################
    #                    PART 4  Scheduler & Optimizer                    #
    #######################################################################
    # optimizer
    optim_wrapper = dict(
        type=AmpOptimWrapper,
        optimizer=dict(
            type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
        clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
        accumulative_counts=accumulative_counts,
        loss_scale='dynamic',
        dtype='float16')

    # learning policy
    # More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
    param_scheduler = dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        T_max=max_epochs,
        convert_to_iter_based=True)

    # train, val, test setting
    train_cfg = dict(by_epoch=True, max_epochs=max_epochs, val_interval=1)

    #######################################################################
    #                           PART 5  Runtime                           #
    #######################################################################
    # Log the dialogue periodically during the training process, optional
    custom_hooks = [
        dict(type=DatasetInfoHook, tokenizer=tokenizer),
        dict(
            type=EvaluateChatHook,
            tokenizer=tokenizer,
            every_n_iters=evaluation_freq,
            evaluation_inputs=evaluation_inputs,
            system=SYSTEM,
            prompt_template=prompt_template)
    ]

    # configure default hooks
    default_hooks = dict(
        # record the time of every iteration.
        timer=dict(type=IterTimerHook),
        # print log every 100 iterations.
        logger=dict(type=LoggerHook, interval=10),
        # enable the parameter scheduler.
        param_scheduler=dict(type=ParamSchedulerHook),
        # save checkpoint per epoch.
        checkpoint=dict(type=CheckpointHook, interval=1),
        # set sampler seed in distributed evrionment.
        sampler_seed=dict(type=DistSamplerSeedHook),
    )

    # configure environment
    env_cfg = dict(
        # whether to enable cudnn benchmark
        cudnn_benchmark=False,
        # set multi process parameters
        mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
        # set distributed parameters
        dist_cfg=dict(backend='nccl'),
    )

    # set visualizer
    visualizer = None

    # set log level
    log_level = 'INFO'

    # load from which checkpoint
    load_from = None

    # whether to resume training from the loaded checkpoint
    resume = False

    # Defaults to use random seed and disable `deterministic`
    randomness = dict(seed=None, deterministic=False)
    ```

### å¾®è°ƒ

```bash
xtuner train /root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py
```

### å‚æ•°è½¬æ¢/åˆå¹¶

- è®­ç»ƒåçš„pthæ ¼å¼å‚æ•°è½¬Hugging Faceæ ¼å¼

```bash
# åˆ›å»ºç”¨äºå­˜æ”¾Hugging Faceæ ¼å¼å‚æ•°çš„hfæ–‡ä»¶å¤¹
mkdir /root/personal_assistant/config/work_dirs/hf

export MKL_SERVICE_FORCE_INTEL=1

# é…ç½®æ–‡ä»¶å­˜æ”¾çš„ä½ç½®
export CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py

# æ¨¡å‹è®­ç»ƒåå¾—åˆ°çš„pthæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
export PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth

# pthæ–‡ä»¶è½¬æ¢ä¸ºHugging Faceæ ¼å¼åå‚æ•°å­˜æ”¾çš„ä½ç½®
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf

# æ‰§è¡Œå‚æ•°è½¬æ¢
xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH
```

- Mergeæ¨¡å‹å‚æ•°

```bash
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER='GNU'

# åŸå§‹æ¨¡å‹å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b

# Hugging Faceæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf

# æœ€ç»ˆMergeåçš„å‚æ•°å­˜æ”¾çš„ä½ç½®
mkdir /root/personal_assistant/config/work_dirs/hf_merge
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge

# æ‰§è¡Œå‚æ•°Merge
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```

### Demo

- å®‰è£…ä¾èµ–

```bash
pip install streamlit==1.24.0
```

- ä¸‹è½½InternLMé¡¹ç›®ä»£ç 

```bash
# åˆ›å»ºcodeæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾InternLMé¡¹ç›®ä»£ç 
mkdir /root/personal_assistant/code && cd /root/personal_assistant/code
git clone https://github.com/InternLM/InternLM.git
```

- å°† `/root/code/InternLM/web_demo.py` ä¸­ 29 è¡Œ å’Œ 33 è¡Œçš„æ¨¡å‹è·¯å¾„æ›´æ¢ä¸ºMergeåå­˜æ”¾å‚æ•°çš„è·¯å¾„ `/root/personal_assistant/config/work_dirs/hf_merge`

- è¿è¡Œ `/root/personal_assistant/code/InternLM` ç›®å½•ä¸‹çš„ `web_demo.py` æ–‡ä»¶ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°

```bash
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

### æ•ˆæœ

- **å¾®è°ƒå‰**ï¼ˆå›ç­”æ¯”è¾ƒå®˜æ–¹ï¼‰

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/hw_4_1.jpg"></center>

- **å¾®è°ƒå**ï¼ˆå¯¹è‡ªå·±çš„èº«ä»½æœ‰äº†æ¸…æ™°çš„è®¤çŸ¥ï¼‰

<center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/hw_4_2.jpg"></center>

## è¿›é˜¶ä½œä¸š

- å°†è®­ç»ƒå¥½çš„Adapteræ¨¡å‹æƒé‡ä¸Šä¼ åˆ° OpenXLabã€Hugging Face æˆ–è€… MoelScope ä»»ä¸€ä¸€å¹³å°ã€‚

    1. é€šè¿‡ scp å°† hf æ–‡ä»¶å¤¹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¿›å…¥ HuggingFace æ–°å»º modelï¼Œä¸Šä¼ æ¨¡å‹æ–‡ä»¶ã€‚Adapter æƒé‡åœ°å€ï¼šhttps://huggingface.co/jujimeizuo/assistant-jujimeizuo/tree/main

    <center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/hw_4_3.jpg"></center>


- å°†è®­ç»ƒå¥½åçš„æ¨¡å‹åº”ç”¨éƒ¨ç½²åˆ° OpenXLab å¹³å°ï¼Œå‚è€ƒéƒ¨ç½²æ–‡æ¡£è¯·è®¿é—®ï¼šhttps://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe

    1. é€šè¿‡ openxlab åº“ä¸Šä¼ æ¨¡å‹æ–‡ä»¶

    <center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/hw_4_4.jpg"></center>

    2. æ–°å»º github çš„ repo([assistant_jujimeizuo](https://github.com/jujimeizuo/assistant_jujimeizuo))ï¼Œä» OpenXLab ä¸Šæ‹‰å–ä¸Šä¼ çš„æ¨¡å‹ï¼Œæ„å»º streamlit çš„ appï¼ŒåŒæ­¥åˆ° OpenXLab ä¸Šã€‚

    3. åº”ç”¨æœåŠ¡åœ°å€ï¼šhttps://openxlab.org.cn/apps/detail/jujimeizuo/assistant_jujimeizuo

    <center><img src="https://cdn.jujimeizuo.cn/note/llm/internlm/hw_4_5.jpg"></center>
